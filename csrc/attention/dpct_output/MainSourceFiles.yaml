---
MainSourceFile:  MainSrcFiles_placehold
Replacements:
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          986
    Length:          0
    ReplacementText: "#include <sycl/sycl.hpp>\n#include <dpct/dpct.hpp>\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1016
    Length:          30
    ReplacementText: '#include "attention_utils.dp.hpp"'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1379
    Length:          11
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1432
    Length:          0
    ReplacementText: ",\n                       const sycl::nd_item<3> &item_ct1"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1499
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1537
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1666
    Length:          0
    ReplacementText: "    /*\n    DPCT1023:10: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_xor_sync.\n    */\n    /*\n    DPCT1064:11: Migrated __shfl_xor_sync call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.\n    */\n    /*\n    DPCT1096:55: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::permute_sub_group_by_xor\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1677
    Length:          29
    ReplacementText: 'dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(), sum, mask)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          1861
    Length:          15
    ReplacementText: "/*\n  DPCT1065:35: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          2088
    Length:          0
    ReplacementText: "    /*\n    DPCT1023:12: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_xor_sync.\n    */\n    /*\n    DPCT1096:56: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::permute_sub_group_by_xor\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          2099
    Length:          29
    ReplacementText: 'dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(), sum, mask)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          2168
    Length:          0
    ReplacementText: "  /*\n  DPCT1023:13: The SYCL sub-group does not support mask options for dpct::select_from_sub_group. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_sync.\n  */\n  /*\n  DPCT1096:57: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::select_from_sub_group\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n  */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          2177
    Length:          22
    ReplacementText: 'dpct::select_from_sub_group(item_ct1.get_sub_group(), sum, 0)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          2458
    Length:          11
    ReplacementText: "/*\nDPCT1110:14: The total declared local variable size in device function paged_attention_kernel exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total register size available and adjust the code, or use smaller sub-group size to avoid high register pressure.\n*/\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          3427
    Length:          0
    ReplacementText: ",\n  const sycl::nd_item<3> &item_ct1,\n  uint8_t *dpct_local,\n  sycl::local_accessor<Q_vec, 2> q_vecs,\n  float *red_smem"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          3453
    Length:          10
    ReplacementText: 'item_ct1.get_group(1)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          3493
    Length:          10
    ReplacementText: 'item_ct1.get_group(0)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          3538
    Length:          9
    ReplacementText: 'item_ct1.get_group_range(0)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          4994
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          5121
    Length:          10
    ReplacementText: 'item_ct1.get_group(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          5157
    Length:          9
    ReplacementText: 'item_ct1.get_group_range(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          6632
    Length:          64
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          6969
    Length:          15
    ReplacementText: "/*\n  DPCT1065:36: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          7111
    Length:          36
    ReplacementText: 'auto shared_mem = (char *)dpct_local;'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          7311
    Length:          41
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10070
    Length:          17
    ReplacementText: 'sycl::fmax(qk_max, qk)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10407
    Length:          0
    ReplacementText: "    /*\n    DPCT1023:15: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_xor_sync.\n    */\n    /*\n    DPCT1064:16: Migrated __shfl_xor_sync call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.\n    */\n    /*\n    DPCT1096:51: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::permute_sub_group_by_xor\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10420
    Length:          47
    ReplacementText: 'sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(), qk_max, mask))'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10531
    Length:          15
    ReplacementText: "/*\n  DPCT1065:37: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10761
    Length:          0
    ReplacementText: "    /*\n    DPCT1023:17: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_xor_sync.\n    */\n    /*\n    DPCT1096:52: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::permute_sub_group_by_xor\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10774
    Length:          47
    ReplacementText: 'sycl::fmax(qk_max, dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(), qk_max, mask))'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10875
    Length:          0
    ReplacementText: "  /*\n  DPCT1023:18: The SYCL sub-group does not support mask options for dpct::select_from_sub_group. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_sync.\n  */\n  /*\n  DPCT1064:19: Migrated __shfl_sync call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.\n  */\n  /*\n  DPCT1096:53: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::select_from_sub_group\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n  */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          10886
    Length:          25
    ReplacementText: 'dpct::select_from_sub_group(item_ct1.get_sub_group(), qk_max, 0)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          11052
    Length:          26
    ReplacementText: 'sycl::native::exp(logits[i] - qk_max)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          11187
    Length:          0
    ReplacementText: ', item_ct1'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          11237
    Length:          32
    ReplacementText: '1.f / (exp_sum + 1e-6f)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          11366
    Length:          15
    ReplacementText: "/*\n  DPCT1065:38: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          14700
    Length:          0
    ReplacementText: "      /*\n      DPCT1023:20: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_xor_sync.\n      */\n      /*\n      DPCT1064:21: Migrated __shfl_xor_sync call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.\n      */\n      /*\n      DPCT1096:54: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::permute_sub_group_by_xor\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n      */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          14713
    Length:          29
    ReplacementText: 'dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(), acc, mask)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          14891
    Length:          15
    ReplacementText: "/*\n  DPCT1065:39: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          15504
    Length:          0
    ReplacementText: "    /*\n    DPCT1118:22: SYCL group functions and algorithms must be encountered in converged control flow. You may need to adjust the code.\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          15508
    Length:          15
    ReplacementText: "/*\n    DPCT1065:40: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n    */\n    item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          15924
    Length:          0
    ReplacementText: "    /*\n    DPCT1118:23: SYCL group functions and algorithms must be encountered in converged control flow. You may need to adjust the code.\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          15928
    Length:          15
    ReplacementText: "/*\n    DPCT1065:41: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n    */\n    item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          16617
    Length:          11
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          17395
    Length:          0
    ReplacementText: ",\n  const sycl::nd_item<3> &item_ct1,\n  uint8_t *dpct_local,\n  sycl::local_accessor<Q_vec, 2> q_vecs,\n  float *red_smem"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          17687
    Length:          0
    ReplacementText: ', item_ct1, dpct_local, q_vecs, red_smem'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          17852
    Length:          11
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          18824
    Length:          0
    ReplacementText: ",\n  const sycl::nd_item<3> &item_ct1,\n  uint8_t *dpct_local,\n  sycl::local_accessor<Q_vec, 2> q_vecs,\n  float *red_smem"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          19108
    Length:          0
    ReplacementText: ', item_ct1, dpct_local, q_vecs, red_smem'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          19235
    Length:          11
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          19721
    Length:          0
    ReplacementText: ",\n  const sycl::nd_item<3> &item_ct1,\n  uint8_t *dpct_local,\n  float *red_smem"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          19749
    Length:          9
    ReplacementText: 'item_ct1.get_group_range(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          19783
    Length:          10
    ReplacementText: 'item_ct1.get_group(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          19817
    Length:          10
    ReplacementText: 'item_ct1.get_group(1)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          20321
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          20354
    Length:          10
    ReplacementText: 'item_ct1.get_local_range(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          20537
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          20581
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          20640
    Length:          36
    ReplacementText: 'auto shared_mem = (char *)dpct_local;'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          20709
    Length:          41
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21065
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21103
    Length:          10
    ReplacementText: 'item_ct1.get_local_range(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21202
    Length:          19
    ReplacementText: 'sycl::fmax(max_logit, (float)l)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21229
    Length:          15
    ReplacementText: "/*\n  DPCT1065:42: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21379
    Length:          0
    ReplacementText: "    /*\n    DPCT1023:24: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_xor_sync.\n    */\n    /*\n    DPCT1064:25: Migrated __shfl_xor_sync call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.\n    */\n    /*\n    DPCT1096:58: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::permute_sub_group_by_xor\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21395
    Length:          53
    ReplacementText: 'sycl::fmax(max_logit, dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(), max_logit, mask))'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21515
    Length:          15
    ReplacementText: "/*\n  DPCT1065:43: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21690
    Length:          0
    ReplacementText: "    /*\n    DPCT1023:26: The SYCL sub-group does not support mask options for dpct::permute_sub_group_by_xor. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_xor_sync.\n    */\n    /*\n    DPCT1096:59: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::permute_sub_group_by_xor\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n    */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21706
    Length:          53
    ReplacementText: 'sycl::fmax(max_logit, dpct::permute_sub_group_by_xor(item_ct1.get_sub_group(), max_logit, mask))'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21810
    Length:          0
    ReplacementText: "  /*\n  DPCT1023:27: The SYCL sub-group does not support mask options for dpct::select_from_sub_group. You can specify \"--use-experimental-features=masked-sub-group-operation\" to use the experimental helper function to migrate __shfl_sync.\n  */\n  /*\n  DPCT1064:28: Migrated __shfl_sync call is used in a macro/template definition and may not be valid for all macro/template uses. Adjust the code.\n  */\n  /*\n  DPCT1096:60: The right-most dimension of the work-group used in the SYCL kernel that calls this function may be less than \"32\". The function \"dpct::select_from_sub_group\" may return an unexpected result on the CPU device. Modify the size of the work-group to ensure that the value of the right-most dimension is a multiple of \"32\".\n  */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          21824
    Length:          28
    ReplacementText: 'dpct::select_from_sub_group(item_ct1.get_sub_group(), max_logit, 0)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          22199
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          22237
    Length:          10
    ReplacementText: 'item_ct1.get_local_range(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          22334
    Length:          19
    ReplacementText: 'sycl::native::exp(l - max_logit)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          22444
    Length:          15
    ReplacementText: "/*\n  DPCT1065:44: Consider replacing sycl::nd_item::barrier() with sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there is no access to global memory.\n  */\n  item_ct1.barrier()"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          22537
    Length:          0
    ReplacementText: ', item_ct1'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          22575
    Length:          40
    ReplacementText: '1.0f / (global_exp_sum + 1e-6f)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          22943
    Length:          11
    ReplacementText: 'item_ct1.get_local_id(2)'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          23224
    Length:          0
    ReplacementText: "/*\nDPCT1049:29: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.\n*/\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          23322
    Length:          210
    ReplacementText: '0'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          23610
    Length:          1361
    ReplacementText: "stream->submit(\\\n  [&](sycl::handler &cgh) {\\\n    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(sycl::range<1>(shared_mem_size), cgh);\\\n    sycl::local_accessor<Q_vec, 2> q_vecs_acc_ct1(sycl::range<2>(THREAD_GROUP_SIZE, NUM_VECS_PER_THREAD), cgh);\\\n    sycl::local_accessor<float, 1> red_smem_acc_ct1(sycl::range<1>(2 * NUM_WARPS), cgh);\\\n\\\n    T * out_ptr_ct0 = out_ptr;\\\n    T * query_ptr_ct1 = query_ptr;\\\n    T * key_cache_ptr_ct2 = key_cache_ptr;\\\n    T * value_cache_ptr_ct3 = value_cache_ptr;\\\n    auto num_kv_heads_ct4 = num_kv_heads;\\\n    auto scale_ct5 = scale;\\\n    int * block_tables_ptr_ct6 = block_tables_ptr;\\\n    int * context_lens_ptr_ct7 = context_lens_ptr;\\\n    auto max_num_blocks_per_seq_ct8 = max_num_blocks_per_seq;\\\n    const float * alibi_slopes_ptr_ct9 = alibi_slopes_ptr;\\\n    auto q_stride_ct10 = q_stride;\\\n    auto kv_block_stride_ct11 = kv_block_stride;\\\n    auto kv_head_stride_ct12 = kv_head_stride;\\\n\\\n    cgh.parallel_for(\\\n      sycl::nd_range<3>(grid * block, block), \\\n      [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {\\\n        vllm::paged_attention_v1_kernel<T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS>(out_ptr_ct0, query_ptr_ct1, key_cache_ptr_ct2, value_cache_ptr_ct3, num_kv_heads_ct4, scale_ct5, block_tables_ptr_ct6, context_lens_ptr_ct7, max_num_blocks_per_seq_ct8, alibi_slopes_ptr_ct9, q_stride_ct10, kv_block_stride_ct11, kv_head_stride_ct12, item_ct1, dpct_local_acc_ct1.get_pointer(), q_vecs_acc_ct1, red_smem_acc_ct1.get_pointer());\\\n      });\\\n  });"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: true
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          24971
    Length:          1
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          26438
    Length:          0
    ReplacementText: "  /*\n  DPCT1083:30: The size of local memory in the migrated code may be different from the original code. Check that the allocated memory size in the migrated code is correct.\n  */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          26751
    Length:          4
    ReplacementText: 'sycl::range<3>'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          26761
    Length:          22
    ReplacementText: 1, num_seqs, num_heads
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          26788
    Length:          4
    ReplacementText: 'sycl::range<3>'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          26799
    Length:          11
    ReplacementText: 1, 1, NUM_THREADS
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          26821
    Length:          12
    ReplacementText: 'dpct::queue_ptr'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          27128
    Length:          0
    ReplacementText: "      /*\n      DPCT1027:45: The call to cudaFuncSetAttribute was replaced with 0 because SYCL currently does not support corresponding setting.\n      */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          27191
    Length:          0
    ReplacementText: "      /*\n      DPCT1027:46: The call to cudaFuncSetAttribute was replaced with 0 because SYCL currently does not support corresponding setting.\n      */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          27254
    Length:          0
    ReplacementText: "      /*\n      DPCT1027:47: The call to cudaFuncSetAttribute was replaced with 0 because SYCL currently does not support corresponding setting.\n      */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          27318
    Length:          0
    ReplacementText: "      /*\n      DPCT1027:48: The call to cudaFuncSetAttribute was replaced with 0 because SYCL currently does not support corresponding setting.\n      */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          27383
    Length:          0
    ReplacementText: "      /*\n      DPCT1027:49: The call to cudaFuncSetAttribute was replaced with 0 because SYCL currently does not support corresponding setting.\n      */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          27448
    Length:          0
    ReplacementText: "      /*\n      DPCT1027:50: The call to cudaFuncSetAttribute was replaced with 0 because SYCL currently does not support corresponding setting.\n      */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          30361
    Length:          13
    ReplacementText: 'sycl::ext::oneapi::bfloat16'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          30461
    Length:          0
    ReplacementText: "/*\nDPCT1049:31: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.\n*/\n/*\nDPCT1049:33: The work-group size passed to the SYCL kernel may exceed the limit. To get the device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.\n*/\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          30559
    Length:          1553
    ReplacementText: "stream->submit(\\\n  [&](sycl::handler &cgh) {\\\n    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(sycl::range<1>(shared_mem_size), cgh);\\\n    sycl::local_accessor<Q_vec, 2> q_vecs_acc_ct1(sycl::range<2>(THREAD_GROUP_SIZE, NUM_VECS_PER_THREAD), cgh);\\\n    sycl::local_accessor<float, 1> red_smem_acc_ct1(sycl::range<1>(2 * NUM_WARPS), cgh);\\\n\\\n    float * exp_sums_ptr_ct0 = exp_sums_ptr;\\\n    float * max_logits_ptr_ct1 = max_logits_ptr;\\\n    T * tmp_out_ptr_ct2 = tmp_out_ptr;\\\n    T * query_ptr_ct3 = query_ptr;\\\n    T * key_cache_ptr_ct4 = key_cache_ptr;\\\n    T * value_cache_ptr_ct5 = value_cache_ptr;\\\n    auto num_kv_heads_ct6 = num_kv_heads;\\\n    auto scale_ct7 = scale;\\\n    int * block_tables_ptr_ct8 = block_tables_ptr;\\\n    int * context_lens_ptr_ct9 = context_lens_ptr;\\\n    auto max_num_blocks_per_seq_ct10 = max_num_blocks_per_seq;\\\n    const float * alibi_slopes_ptr_ct11 = alibi_slopes_ptr;\\\n    auto q_stride_ct12 = q_stride;\\\n    auto kv_block_stride_ct13 = kv_block_stride;\\\n    auto kv_head_stride_ct14 = kv_head_stride;\\\n\\\n    cgh.parallel_for(\\\n      sycl::nd_range<3>(grid * block, block), \\\n      [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {\\\n        vllm::paged_attention_v2_kernel<T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS, PARTITION_SIZE>(exp_sums_ptr_ct0, max_logits_ptr_ct1, tmp_out_ptr_ct2, query_ptr_ct3, key_cache_ptr_ct4, value_cache_ptr_ct5, num_kv_heads_ct6, scale_ct7, block_tables_ptr_ct8, context_lens_ptr_ct9, max_num_blocks_per_seq_ct10, alibi_slopes_ptr_ct11, q_stride_ct12, kv_block_stride_ct13, kv_head_stride_ct14, item_ct1, dpct_local_acc_ct1.get_pointer(), q_vecs_acc_ct1, red_smem_acc_ct1.get_pointer());\\\n      });\\\n  });"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: true
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          32112
    Length:          1
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          32191
    Length:          693
    ReplacementText: "stream->submit(\\\n  [&](sycl::handler &cgh) {\\\n    sycl::local_accessor<uint8_t, 1> dpct_local_acc_ct1(sycl::range<1>(reduce_shared_mem_size), cgh);\\\n    sycl::local_accessor<float, 1> red_smem_acc_ct1(sycl::range<1>(2 * NUM_WARPS), cgh);\\\n\\\n    T * out_ptr_ct0 = out_ptr;\\\n    float * exp_sums_ptr_ct1 = exp_sums_ptr;\\\n    float * max_logits_ptr_ct2 = max_logits_ptr;\\\n    T * tmp_out_ptr_ct3 = tmp_out_ptr;\\\n    int * context_lens_ptr_ct4 = context_lens_ptr;\\\n    auto max_num_partitions_ct5 = max_num_partitions;\\\n\\\n    cgh.parallel_for(\\\n      sycl::nd_range<3>(reduce_grid * block, block), \\\n      [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {\\\n        vllm::paged_attention_v2_reduce_kernel<T, HEAD_SIZE, NUM_THREADS, PARTITION_SIZE>(out_ptr_ct0, exp_sums_ptr_ct1, max_logits_ptr_ct2, tmp_out_ptr_ct3, context_lens_ptr_ct4, max_num_partitions_ct5, item_ct1, dpct_local_acc_ct1.get_pointer(), red_smem_acc_ct1.get_pointer());\\\n      });\\\n  });"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: true
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          32884
    Length:          1
    ReplacementText: ''
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          34620
    Length:          0
    ReplacementText: "  /*\n  DPCT1083:34: The size of local memory in the migrated code may be different from the original code. Check that the allocated memory size in the migrated code is correct.\n  */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          34777
    Length:          4
    ReplacementText: 'sycl::range<3>'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          34787
    Length:          39
    ReplacementText: max_num_partitions, num_seqs, num_heads
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          34935
    Length:          4
    ReplacementText: 'sycl::range<3>'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          34952
    Length:          19
    ReplacementText: 1, num_seqs, num_heads
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          34974
    Length:          0
    ReplacementText: "  /*\n  DPCT1083:32: The size of local memory in the migrated code may be different from the original code. Check that the allocated memory size in the migrated code is correct.\n  */\n"
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          35048
    Length:          4
    ReplacementText: 'sycl::range<3>'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          35059
    Length:          11
    ReplacementText: 1, 1, NUM_THREADS
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          35081
    Length:          12
    ReplacementText: 'dpct::queue_ptr'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
  - FilePath:        '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Offset:          39079
    Length:          13
    ReplacementText: 'sycl::ext::oneapi::bfloat16'
    ConstantFlag:    ''
    ConstantOffset:  0
    InitStr:         ''
    NewHostVarName:  ''
    BlockLevelFormatFlag: false
MainSourceFilesDigest:
  - MainSourceFile:  '/nfs/site/home/kminemur/projects/vllm/csrc/attention/attention_kernels.cu'
    Digest:          0095293be80b1d0cd086bef6effb77f1
DpctVersion:     2024.0.0
MainHelperFileName: ''
USMLevel:        ''
FeatureMap:      {}
CompileTargets:  {}
OptionMap:
  AnalysisScopePath:
    Value:           '/nfs/site/home/kminemur/projects/vllm/csrc/attention'
    Specified:       false
  AsyncHandler:
    Value:           'false'
    Specified:       false
  CommentsEnabled:
    Value:           'false'
    Specified:       false
  CompilationsDir:
    Value:           ''
    Specified:       false
  CtadEnabled:
    Value:           'false'
    Specified:       false
  EnablepProfiling:
    Value:           'false'
    Specified:       false
  ExperimentalFlag:
    Value:           '0'
    Specified:       false
  ExplicitClNamespace:
    Value:           'false'
    Specified:       false
  ExplicitNamespace:
    Value:           '20'
    Specified:       false
  ExtensionDDFlag:
    Value:           '0'
    Specified:       false
  ExtensionDEFlag:
    Value:           '4294967295'
    Specified:       false
  HelperFuncPreferenceFlag:
    Value:           '0'
    Specified:       false
  NDRangeDim:
    Value:           '3'
    Specified:       false
  NoDRYPattern:
    Value:           'false'
    Specified:       false
  NoUseGenericSpace:
    Value:           ''
    Specified:       true
  OptimizeMigration:
    Value:           'false'
    Specified:       false
  ProcessAll:
    Value:           'false'
    Specified:       false
  RuleFile:
    Value:           ''
    Specified:       false
  SyclNamedLambda:
    Value:           'false'
    Specified:       false
  UsmLevel:
    Value:           '1'
    Specified:       false
...
